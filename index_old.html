<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Embodied Image Captioning</title>
    <meta name="author" content="embodied-captioning"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="description" content=""/>
    <meta name="keywords" content=""/>
    <meta property="og:title" content="Title"/>
    <meta property="og:description" content=""/>
    <meta property="og:image" content="assets/.png"/>

    <script src="js/init.js"></script>

    <link rel="stylesheet" href="css/mystyle.css">
    <link rel="stylesheet" href="css/academicons.css">
    <link rel="stylesheet" href="css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
<div>
    <h1>Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</h1>
    <h3>T. Galliena, T. Apicella, S. Rosa, P. Morerio, A. Del Bue, L. Natale</h3>
    Istituto Italiano di Tecnologia, Genoa, Italy
    <br>
    <br>
    <br>
</div>

<div class="links">
    <!-- <a href="" class="icon_publication"><i class="fa fa-file-pdf-o" style="font-size:20px;"> Paper</i></a> -->
    <a href="https://doi.org/10.48550/arXiv.2504.08531" class="icon_publication"><i class="ai ai-arxiv ai-3x"
                                              style="font-size:20px;"> arXiv</i></a>
    <a href="https://github.com/hsp-iit/embodied-captioning" class="icon_publication"><i class="fa fa-github"
                                              style="font-size:20px;"> Code</i></a>
</div>

<br>

<div class="image">
    <img src="assets/framework.png"/>
</div>

<div class="text">
    <p>
        We present a self-supervised method to improve an agent's abilities in describing arbitrary objects
        while actively exploring a generic environment. This is a challenging problem, as current models struggle to
        obtain coherent image captions due to different camera viewpoints and clutter.
        We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and
        consistency across views via a consensus mechanism.
        First, an agent explores the environment, collecting noisy image-caption pairs.
        Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language
        model.
        Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of
        contrastive learning.
        We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling
        methods, and fine-tuning strategies, on our manually labeled test set.
        Results show that a policy can be trained to mine samples with higher disagreement compared to classical
        baselines.
        Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to
        other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin.
    </p>
</div>
<br>
<div>
    <h2 class="section">Approach</h2>
    <div class="text">
        <h3>Exploration</h3>
        <p>
            The agent navigates the environment using any exploration policy chosen by the user, allowing full flexibility in how observations are collected. 
            In addition to this, we train a reinforcement learning (RL) policy that explicitly encourages the agent to collect object samples 
            where the captioning model exhibits high disagreement across different viewpoints. 
            This targeted exploration improves the quality and diversity of the collected dataset, focusing on challenging and informative samples.
        </p>

        <h3>Pseudo-captioning</h3>
        <p>
            From the collected image-caption pairs, we apply a consensus mechanism 
            using a large language model to distill a consistent pseudo-caption for each object instance, 
            reducing noise from different viewpoints and descriptions.
        </p>

        <h3>Fine-tuning</h3>
        <p>
            The distilled pseudo-captions are used to fine-tune an existing captioning model. 
            We additionally apply contrastive learning techniques to further improve consistency and semantic accuracy across multiple views of the same object.
        </p>
        <br><br>
        <h3>Method Overview Video</h3>
            <div class="video">
                <iframe width="720" height="405"
                    src="https://www.youtube.com/embed/tmUUT-938Xk" 
                    title="Method Overview Video"
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                </iframe>
            </div>
        <br><br>
    </div>
</div>
<br>
<br>
<div>
    <h2 class="section">Reference</h2>
    <div class="text">
        If you use the information in the paper please cite the following reference.<br><br>
        Plain text format
        <pre> T. Galliena, T. Apicella, S. Rosa, P. Morerio, A. Del Bue, L. Natale, <em>Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</em>,
arXiv preprint arXiv:2504.08531, 2025
        </pre>
        <br>
        Bibtex format
        <pre>@article{galliena2025embodied,
        title={Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions},
        author={Galliena, Tommaso and Apicella, Tommaso and Rosa, Stefano and Morerio, Pietro and Del Bue, Alessio and Natale, Lorenzo},
        journal={arXiv preprint arXiv:2504.08531},
        year={2025}
}
        </pre>
    </div>
</div>

<br>

<h2 class="section">Contact</h2>
<div class="text">
    If you have any further enquiries, question, or comments, please open an issue on the Github repository page.
    <br>
    <br>
</div>
<br><br>

<!-- Featured -->
<div id="featured">
    <div class="divider"></div>
</div>
<!-- /Featured -->

<div id="footer">
    <div class="container">
    </div>
</div>
<br><br>
</body>
</html>
